All the files and scripts in this folder are for automatically downloading, processing, and uploading data from the NHGRI-EBI GWAS Catalog. The following 
is a tutorial on what each script does and how they work together.

To update the PRSKB database automatically with the latest studies from the NHGRI-EBI GWAS Catalog, follow the following steps:
1. ssh into the PRSKB server at lsprs.byu.edu with your username and password.
2. Navigate to the scripts folder where the website is stored: cd /var/www/prs.byu.edu/html/update_database_scripts/
3. Open a new tmux window by typing the following command: "tmux new -s session_name" where session_name is the name of the session you are starting.
   Opening a tmux window allows the scripts to run even when you are not logged onto the server.
    a. If tmux is not available, install it with the following command: "sudo apt-get install tmux"
    b. You can leave or come back to a tmux window at any time with "tmux detach" and "tmux a -t session_name" respectively. 
4. Run the following command, replacing "password" with the password for the PRSKB MySQL password: sudo ./master_script.sh "password" 8 &> output.txt &
    a. After running the command, make sure the script has started working by opening the output.txt file and confirming there are no errors.
        aa. If you get the error: "./master_script.sh: Permission denied" then do "chmod 777 master_script"
    b. See the master_script file for additional information on its parameters.
    c. This command will run in the background and write its terminal output to "output.txt" Remember, while in a tmux window, run "tmux detach" and the 
       command will continue to run in the detached tmux window.
    d. The "8" in the command is the number of times the GWAS catalog will be split to download different parts of the database concurrently. 
       Increase this number to speed up the download, but don't increase the number too much since the website computer only has 2 cores.
5. This command will take at least 4 hours to run. It is done when the last line of "output.txt" says "Press [Enter] key to finish..." At this point, 
   all of the new data is already uploaded to the database and and you just need to upload the new TSVs to Github.
6. Upload the new TSVs to github: (This may be another script in the future)
    a. git add .
    b. git commit -m "updated database *MM/DD/YY*"
    c. git push origin master
7. If you run the master script, but then want to stop it, you can use the following process:
    a. When you ran the "./master_script.sh" command, it should have returned a number. This number is the PID, or the process ID for the background
       process you started. To stop it, do: sudo kill "PID" where PID is the number returned.
    b. If you have somehow lost the PID mentioned in the previous step, you can find it with the following process:
        i. Run: sudo ps -aux | less
        ii. Scroll until you find your command under the "COMMAND" column (choose the one that starts with "sudo" not "/bin/bash") Note the PID.
        iii. Exit "less" with "q" 
        iv. Execute sudo kill "PID" where PID is the number you found.

The following is a simplified description of the files in the update_database_scripts folder. They are listed in the order they are used when running 
the master_script. Please see the beginning of each of the files for command line parameters, usage, and additional information.

master_script.sh- A shell script that calls the other files in the update_database_scripts folder to download, format, and upload new data from the 
    GWAS catalog to the PRSKB database. See the file for details on command line arguments.
	
downloadStudiesToFile.R- An R script that downloads study, publication, and ancestry data for all GWAS catalog studies so each instance of the
	unpackDatabaseCommandLine.R doesn't have to. These tables are written out, as TSVs, but later deleted by the master_script after use.

unpackDatabaseCommandLine.R- An R script that downloads data from the GWAS catalog and formats it into a TSV table (associations_table.tsv) that can 
	be uploaded to the PRSKB database. This script can be run multiple times simultaneously on different portions of the GWAS catalog so the data can
	be downloaded faster. This script can also be used as a stand-alone script.

hg19ToHg17.over.chain, hg19ToHg18.over.chain, and hg38ToHg19.over.chain- These are three chain files used to convert coordinate systems between 
    reference genomes by the unpackDatabaseCommandLine.R script. The GWAS catalog only gives SNP locations in hg38, but these chain files allow the 
    unpackDatabaseCommandLine.R to convert hg38 locations to hg19, hg18, and hg17.

sortAssociationsTable.py- A Python script that sorts the associations table after it has been generated by the multiple instances of the 
    unpackDatabaseCommandLine.R script. This script is necessary because it adds the "id" column to the associations table. It cannot be incorporated
    into the unpackDatabaseCommandLine.R script because of the multiple instances of that script that add rows to the associations table at the same time.

createStudyTable.R- This R script takes information from the association table TSV created by the unpackDatabaseCommandLine.R script as well as the 
    GWAS catalog to create a study_table.tsv file. The study table is a summary of all studies in the association table TSV and includes 
    additional information on each study, including the last time it was updated and the name of the study.

uploadTablesToDatabase.py- This Python script uploads the new association table and study table to the PRSKB database. This script can also be run by 
    itself, independently of the master_script.

createSampleVCF.py- A Python script that generates a new sample VCF once all new data has been added to the server. It takes a single SNP from each
    study and has three samples representing ref/ref, ref/alt, and alt/alt alleles.